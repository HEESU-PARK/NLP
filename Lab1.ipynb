{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dfad454",
   "metadata": {},
   "source": [
    "## 필요한 라이브러리 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b893ec55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/parkheesu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/parkheesu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/parkheesu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/parkheesu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from autocorrect import Speller\n",
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a107be1b",
   "metadata": {},
   "source": [
    "## 텍스트 코퍼스를 변수에 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f514042",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = open(\"/Users/parkheesu/Desktop/NLP/file.txt\", 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9276130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/parkheesu/Desktop/NLP\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940460da",
   "metadata": {},
   "source": [
    "## 토큰화 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e744a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9d145b",
   "metadata": {},
   "source": [
    "## 토큰 리스트 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c7af70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'this', 'book', 'authored', 'by', 'Sohom', 'Ghosh', 'and', 'Dwight', 'Gunning', ',', 'we', 'shall', 'learnning', 'how', 'to', 'pracess', 'Natueral', 'Language', 'and']\n"
     ]
    }
   ],
   "source": [
    "print(words[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30075f9f",
   "metadata": {},
   "source": [
    "## 철자 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cc647dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sohom has been corrected to: Show\n",
      "Ghosh has been corrected to: Ghost\n",
      "Dwight has been corrected to: Right\n",
      "Gunning has been corrected to: Running\n",
      "learnning has been corrected to: learning\n",
      "pracess has been corrected to: process\n",
      "Natueral has been corrected to: Natural\n",
      "NLP has been corrected to: LP\n",
      "NLP has been corrected to: LP\n",
      "prajects has been corrected to: projects\n"
     ]
    }
   ],
   "source": [
    "corrected_sentence = \"\"\n",
    "corrected_word_list = []\n",
    "spell = Speller(lang='en')\n",
    "for wd in words:\n",
    "    if wd not in string.punctuation:\n",
    "        wd_c = spell(wd)\n",
    "        if wd_c != wd:\n",
    "            print(wd + \" has been corrected to: \" + wd_c)\n",
    "            corrected_sentence = corrected_sentence + \" \" + wd_c\n",
    "            corrected_word_list.append(wd_c)\n",
    "        else:\n",
    "            corrected_sentence = corrected_sentence + \" \" + wd\n",
    "            corrected_word_list.append(wd)\n",
    "    else:\n",
    "        corrected_sentence = corrected_sentence + wd\n",
    "        corrected_word_list.append(wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd52311",
   "metadata": {},
   "source": [
    "## 수정한 텍스트 코퍼스 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1687ec8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' In this book authored by Show Ghost and Right Running, we shall learning how to process Natural Language and extract insights from it. The first four chapter will introduce you to the basics of LP. Later chapters will describe how to deal with complex LP projects. If you want to get early access of it, you should book your order now.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da55c6a",
   "metadata": {},
   "source": [
    "## 수정한 단어들에 대한 처음 20개 토큰 리스트 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fce077f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'this', 'book', 'authored', 'by', 'Show', 'Ghost', 'and', 'Right', 'Running', ',', 'we', 'shall', 'learning', 'how', 'to', 'process', 'Natural', 'Language', 'and']\n"
     ]
    }
   ],
   "source": [
    "print(corrected_word_list[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8804aa79",
   "metadata": {},
   "source": [
    "## 수정한 단어에 대해 품사(PoS) 태그"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0265f5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('In', 'IN'), ('this', 'DT'), ('book', 'NN'), ('authored', 'VBN'), ('by', 'IN'), ('Show', 'NNP'), ('Ghost', 'NNP'), ('and', 'CC'), ('Right', 'NNP'), ('Running', 'NNP'), (',', ','), ('we', 'PRP'), ('shall', 'MD'), ('learning', 'VB'), ('how', 'WRB'), ('to', 'TO'), ('process', 'VB'), ('Natural', 'NNP'), ('Language', 'NNP'), ('and', 'CC'), ('extract', 'JJ'), ('insights', 'NNS'), ('from', 'IN'), ('it', 'PRP'), ('.', '.'), ('The', 'DT'), ('first', 'JJ'), ('four', 'CD'), ('chapter', 'NN'), ('will', 'MD'), ('introduce', 'VB'), ('you', 'PRP'), ('to', 'TO'), ('the', 'DT'), ('basics', 'NNS'), ('of', 'IN'), ('LP', 'NNP'), ('.', '.'), ('Later', 'NNP'), ('chapters', 'NNS'), ('will', 'MD'), ('describe', 'VB'), ('how', 'WRB'), ('to', 'TO'), ('deal', 'VB'), ('with', 'IN'), ('complex', 'JJ'), ('LP', 'NNP'), ('projects', 'NNS'), ('.', '.'), ('If', 'IN'), ('you', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('get', 'VB'), ('early', 'JJ'), ('access', 'NN'), ('of', 'IN'), ('it', 'PRP'), (',', ','), ('you', 'PRP'), ('should', 'MD'), ('book', 'NN'), ('your', 'PRP$'), ('order', 'NN'), ('now', 'RB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(corrected_word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cea9f04",
   "metadata": {},
   "source": [
    "## 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c25770f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'book',\n",
       " 'authored',\n",
       " 'Show',\n",
       " 'Ghost',\n",
       " 'Right',\n",
       " 'Running',\n",
       " ',',\n",
       " 'shall',\n",
       " 'learning',\n",
       " 'process',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'extract',\n",
       " 'insights',\n",
       " '.',\n",
       " 'The',\n",
       " 'first',\n",
       " 'four',\n",
       " 'chapter']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('English')\n",
    "corrected_word_list_without_stopwords = []\n",
    "for wd in corrected_word_list:\n",
    "    if wd not in stop_words:\n",
    "        corrected_word_list_without_stopwords.append(wd)\n",
    "corrected_word_list_without_stopwords[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d09ab9",
   "metadata": {},
   "source": [
    "## 어간 추출 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd857b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'book',\n",
       " 'author',\n",
       " 'show',\n",
       " 'ghost',\n",
       " 'right',\n",
       " 'run',\n",
       " ',',\n",
       " 'shall',\n",
       " 'learn',\n",
       " 'process',\n",
       " 'natur',\n",
       " 'languag',\n",
       " 'extract',\n",
       " 'insight',\n",
       " '.',\n",
       " 'the',\n",
       " 'first',\n",
       " 'four',\n",
       " 'chapter']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "corrected_word_list_without_stopwords_stemmed = []\n",
    "for wd in corrected_word_list_without_stopwords:\n",
    "    corrected_word_list_without_stopwords_stemmed.append(stemmer.stem(wd))\n",
    "corrected_word_list_without_stopwords_stemmed[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dfc9a8",
   "metadata": {},
   "source": [
    "## 수정된 단어 리스트에 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00184ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'book',\n",
       " 'authored',\n",
       " 'Show',\n",
       " 'Ghost',\n",
       " 'Right',\n",
       " 'Running',\n",
       " ',',\n",
       " 'shall',\n",
       " 'learning',\n",
       " 'process',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'extract',\n",
       " 'insight',\n",
       " '.',\n",
       " 'The',\n",
       " 'first',\n",
       " 'four',\n",
       " 'chapter']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "corrected_word_list_without_stopwords_lemmatized = []\n",
    "for wd in corrected_word_list_without_stopwords:\n",
    "    corrected_word_list_without_stopwords_lemmatized.append(lemmatizer.lemmatize(wd))\n",
    "corrected_word_list_without_stopwords_lemmatized[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f787cd",
   "metadata": {},
   "source": [
    "## 문장 경계 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfe3e8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' In this book authored by Show Ghost and Right Running, we shall learning how to process Natural Language and extract insights from it.', 'The first four chapter will introduce you to the basics of LP.', 'Later chapters will describe how to deal with complex LP projects.', 'If you want to get early access of it, you should book your order now.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(corrected_sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
